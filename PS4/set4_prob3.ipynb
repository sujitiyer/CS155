{"cells":[{"cell_type":"markdown","metadata":{"id":"lqAbFcqaHDcs"},"source":["# Problem 3"]},{"cell_type":"markdown","metadata":{"id":"YULN5VP-HDcu"},"source":["Use this notebook to write your code for problem 3."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQhIwez3HDcv"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"iq4mSQZdHDcw"},"source":["## 3D - Convolutional network"]},{"cell_type":"markdown","metadata":{"id":"I4pyZDOSHDcw"},"source":["As in problem 2, we have conveniently provided for your use code that loads and preprocesses the MNIST data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ztyD6jJfHDcx","executionInfo":{"status":"ok","timestamp":1707368277114,"user_tz":480,"elapsed":168,"user":{"displayName":"Sujit Iyer","userId":"05651463945393873601"}},"outputId":"08c7d92c-78f6-4a12-91fa-e8ad2f54cb62"},"outputs":[{"output_type":"stream","name":"stdout","text":["<torch.utils.data.dataloader.DataLoader object at 0x7b0da8d5b3d0>\n"]}],"source":["# load MNIST data into PyTorch format\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","# set batch size\n","batch_size = 32\n","\n","# load training data downloaded into data/ folder\n","mnist_training_data = torchvision.datasets.MNIST('data/', train=True, download=True,\n","                                                transform=transforms.ToTensor())\n","# transforms.ToTensor() converts batch of images to 4-D tensor and normalizes 0-255 to 0-1.0\n","training_data_loader = torch.utils.data.DataLoader(mnist_training_data,\n","                                                  batch_size=batch_size,\n","                                                  shuffle=True)\n","\n","\n","\n","# load test data\n","mnist_test_data = torchvision.datasets.MNIST('data/', train=False, download=True,\n","                                                transform=transforms.ToTensor())\n","test_data_loader = torch.utils.data.DataLoader(mnist_test_data,\n","                                                  batch_size=batch_size,\n","                                                  shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A9Gaz8WIHDcx","executionInfo":{"status":"ok","timestamp":1706932604232,"user_tz":480,"elapsed":3,"user":{"displayName":"Sujit Iyer","userId":"05651463945393873601"}},"outputId":"bfcc8a28-6ceb-4cd0-9689-b743eee82034"},"outputs":[{"output_type":"stream","name":"stdout","text":["1875 training batches\n","60000 training samples\n","313 validation batches\n"]}],"source":["# look at the number of batches per epoch for training and validation\n","print(f'{len(training_data_loader)} training batches')\n","print(f'{len(training_data_loader) * batch_size} training samples')\n","print(f'{len(test_data_loader)} validation batches')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MyUs54FdHDcy"},"outputs":[],"source":["# sample model\n","import torch.nn as nn\n","\n","model = nn.Sequential(\n","    nn.Conv2d(1, 8, kernel_size=(3,3)),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=0.5),\n","\n","    nn.Conv2d(8, 8, kernel_size=(3,3)),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=0.5),\n","\n","    nn.Flatten(),\n","    nn.Linear(25*8, 64),\n","    nn.ReLU(),\n","    nn.Linear(64, 10)\n","    # PyTorch implementation of cross-entropy loss includes softmax layer\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I0Z2sJ1uHDcy","executionInfo":{"status":"ok","timestamp":1706932604533,"user_tz":480,"elapsed":2,"user":{"displayName":"Sujit Iyer","userId":"05651463945393873601"}},"outputId":"ab37fc47-a4d7-4843-efda-e9184dbb1420"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 1, 3, 3])\n","torch.Size([8])\n","torch.Size([8, 8, 3, 3])\n","torch.Size([8])\n","torch.Size([64, 200])\n","torch.Size([64])\n","torch.Size([10, 64])\n","torch.Size([10])\n"]}],"source":["# why don't we take a look at the shape of the weights for each layer\n","for p in model.parameters():\n","    print(p.data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GYSE7uBcHDcz","executionInfo":{"status":"ok","timestamp":1706932612894,"user_tz":480,"elapsed":265,"user":{"displayName":"Sujit Iyer","userId":"05651463945393873601"}},"outputId":"deb27b6c-4a7e-4f48-c891-ff70cb60cb9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["total params: 14178\n"]}],"source":["# our model has some # of parameters:\n","count = 0\n","for p in model.parameters():\n","    n_params = np.prod(list(p.data.shape)).item()\n","    count += n_params\n","print(f'total params: {count}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RScLsKQ6HDcz"},"outputs":[],"source":["# For a multi-class classification problem\n","import torch.optim as optim\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.RMSprop(model.parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7fBWqfuHHDcz","executionInfo":{"status":"ok","timestamp":1706932864536,"user_tz":480,"elapsed":245678,"user":{"displayName":"Sujit Iyer","userId":"05651463945393873601"}},"outputId":"5c0a55d2-29b9-4f77-afd1-a07e601e5719"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10:...........\n","\tloss: 0.6733, acc: 0.7771, val loss: 0.2138, val acc: 0.9384\n","Epoch 2/10:...........\n","\tloss: 0.4324, acc: 0.8639, val loss: 0.1982, val acc: 0.9456\n","Epoch 3/10:...........\n","\tloss: 0.4195, acc: 0.8689, val loss: 0.1896, val acc: 0.9473\n","Epoch 4/10:...........\n","\tloss: 0.4075, acc: 0.8721, val loss: 0.1888, val acc: 0.9479\n","Epoch 5/10:...........\n","\tloss: 0.3982, acc: 0.8748, val loss: 0.1824, val acc: 0.9503\n","Epoch 6/10:...........\n","\tloss: 0.3965, acc: 0.8769, val loss: 0.1990, val acc: 0.9485\n","Epoch 7/10:...........\n","\tloss: 0.3964, acc: 0.8765, val loss: 0.1912, val acc: 0.9474\n","Epoch 8/10:...........\n","\tloss: 0.3958, acc: 0.8769, val loss: 0.1643, val acc: 0.9541\n","Epoch 9/10:...........\n","\tloss: 0.3920, acc: 0.8793, val loss: 0.1727, val acc: 0.9494\n","Epoch 10/10:...........\n","\tloss: 0.3896, acc: 0.8809, val loss: 0.1970, val acc: 0.9458\n"]}],"source":["# Train the model for 10 epochs, iterating on the data in batches\n","n_epochs = 10\n","\n","# store metrics\n","training_accuracy_history = np.zeros([n_epochs, 1])\n","training_loss_history = np.zeros([n_epochs, 1])\n","validation_accuracy_history = np.zeros([n_epochs, 1])\n","validation_loss_history = np.zeros([n_epochs, 1])\n","\n","for epoch in range(n_epochs):\n","    print(f'Epoch {epoch+1}/10:', end='')\n","    train_total = 0\n","    train_correct = 0\n","    # train\n","    model.train()\n","    for i, data in enumerate(training_data_loader):\n","        images, labels = data\n","        optimizer.zero_grad()\n","        # forward pass\n","        output = model(images)\n","        # calculate categorical cross entropy loss\n","        loss = criterion(output, labels)\n","        # backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # track training accuracy\n","        _, predicted = torch.max(output.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","        # track training loss\n","        training_loss_history[epoch] += loss.item()\n","        # progress update after 180 batches (~1/10 epoch for batch size 32)\n","        if i % 180 == 0: print('.',end='')\n","    training_loss_history[epoch] /= len(training_data_loader)\n","    training_accuracy_history[epoch] = train_correct / train_total\n","    print(f'\\n\\tloss: {training_loss_history[epoch,0]:0.4f}, acc: {training_accuracy_history[epoch,0]:0.4f}',end='')\n","\n","    # validate\n","    test_total = 0\n","    test_correct = 0\n","    with torch.no_grad():\n","        model.eval()\n","        for i, data in enumerate(test_data_loader):\n","            images, labels = data\n","            # forward pass\n","            output = model(images)\n","            # find accuracy\n","            _, predicted = torch.max(output.data, 1)\n","            test_total += labels.size(0)\n","            test_correct += (predicted == labels).sum().item()\n","            # find loss\n","            loss = criterion(output, labels)\n","            validation_loss_history[epoch] += loss.item()\n","        validation_loss_history[epoch] /= len(test_data_loader)\n","        validation_accuracy_history[epoch] = test_correct / test_total\n","    print(f', val loss: {validation_loss_history[epoch,0]:0.4f}, val acc: {validation_accuracy_history[epoch,0]:0.4f}')"]},{"cell_type":"markdown","metadata":{"id":"WizQHkSIHDcz"},"source":["Above, we output the training loss/accuracy as well as the validation loss and accuracy. Not bad! Let's see if you can do better."]},{"cell_type":"markdown","source":["## **PART G**"],"metadata":{"id":"3CcqRKyTIPVC"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","model = nn.Sequential(\n","    nn.Conv2d(1, 32, kernel_size=(3,3)),\n","    nn.BatchNorm2d(num_features=32),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=0.1),\n","\n","    nn.Conv2d(32, 16, kernel_size=(3,3)),\n","    nn.BatchNorm2d(num_features=16),\n","    nn.ReLU(),\n","    nn.Dropout(p=0.1),\n","\n","    nn.Conv2d(16, 8, kernel_size=(3,3)),\n","    nn.BatchNorm2d(num_features=8),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=0.1),\n","\n","    nn.Flatten(),\n","    nn.Linear(128, 64),\n","    nn.BatchNorm1d(num_features=64),\n","    nn.ReLU(),\n","    nn.Linear(64, 10)\n","    # PyTorch implementation of cross-entropy loss includes softmax layer\n",")\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EngeK-feIWhW","executionInfo":{"status":"ok","timestamp":1706945020049,"user_tz":480,"elapsed":258,"user":{"displayName":"Sujit Iyer","userId":"05651463945393873601"}},"outputId":"102ff4ca-5e8c-40e8-bd53-a9c3b457a6aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential(\n","  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n","  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (2): ReLU()\n","  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (4): Dropout(p=0.1, inplace=False)\n","  (5): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n","  (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (7): ReLU()\n","  (8): Dropout(p=0.1, inplace=False)\n","  (9): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1))\n","  (10): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (11): ReLU()\n","  (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (13): Dropout(p=0.1, inplace=False)\n","  (14): Flatten(start_dim=1, end_dim=-1)\n","  (15): Linear(in_features=128, out_features=64, bias=True)\n","  (16): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (17): ReLU()\n","  (18): Linear(in_features=64, out_features=10, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["# why don't we take a look at the shape of the weights for each layer\n","for p in model.parameters():\n","    print(p.data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"khVgDByXLdV5","executionInfo":{"status":"ok","timestamp":1706944253540,"user_tz":480,"elapsed":240,"user":{"displayName":"Sujit Iyer","userId":"05651463945393873601"}},"outputId":"010344cb-5c06-4315-a156-0ffeda60f157"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 1, 3, 3])\n","torch.Size([32])\n","torch.Size([32])\n","torch.Size([32])\n","torch.Size([16, 32, 3, 3])\n","torch.Size([16])\n","torch.Size([16])\n","torch.Size([16])\n","torch.Size([8, 16, 3, 3])\n","torch.Size([8])\n","torch.Size([8])\n","torch.Size([8])\n","torch.Size([64, 128])\n","torch.Size([64])\n","torch.Size([64])\n","torch.Size([64])\n","torch.Size([10, 64])\n","torch.Size([10])\n"]}]},{"cell_type":"code","source":["count = 0\n","for p in model.parameters():\n","    n_params = np.prod(list(p.data.shape)).item()\n","    count += n_params\n","print(f'total params: {count}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bIqr6K2YLmvU","executionInfo":{"status":"ok","timestamp":1706944273109,"user_tz":480,"elapsed":239,"user":{"displayName":"Sujit Iyer","userId":"05651463945393873601"}},"outputId":"229d64a0-c214-4e75-c0ba-9a476e10fc60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total params: 15250\n"]}]},{"cell_type":"code","source":["# For a multi-class classification problem\n","import torch.optim as optim\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.RMSprop(model.parameters())"],"metadata":{"id":"u785rP7dLpNo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model for 10 epochs, iterating on the data in batches\n","n_epochs = 10\n","\n","# store metrics\n","training_accuracy_history = np.zeros([n_epochs, 1])\n","training_loss_history = np.zeros([n_epochs, 1])\n","validation_accuracy_history = np.zeros([n_epochs, 1])\n","validation_loss_history = np.zeros([n_epochs, 1])\n","\n","for epoch in range(n_epochs):\n","    print(f'Epoch {epoch+1}/10:', end='')\n","    train_total = 0\n","    train_correct = 0\n","    # train\n","    model.train()\n","    for i, data in enumerate(training_data_loader):\n","        images, labels = data\n","        optimizer.zero_grad()\n","        # forward pass\n","        output = model(images)\n","        # calculate categorical cross entropy loss\n","        loss = criterion(output, labels)\n","        # backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # track training accuracy\n","        _, predicted = torch.max(output.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","        # track training loss\n","        training_loss_history[epoch] += loss.item()\n","        # progress update after 180 batches (~1/10 epoch for batch size 32)\n","        if i % 180 == 0: print('.',end='')\n","    training_loss_history[epoch] /= len(training_data_loader)\n","    training_accuracy_history[epoch] = train_correct / train_total\n","    print(f'\\n\\tloss: {training_loss_history[epoch,0]:0.4f}, acc: {training_accuracy_history[epoch,0]:0.4f}',end='')\n","\n","    # validate\n","    test_total = 0\n","    test_correct = 0\n","    with torch.no_grad():\n","        model.eval()\n","        for i, data in enumerate(test_data_loader):\n","            images, labels = data\n","            # forward pass\n","            output = model(images)\n","            # find accuracy\n","            _, predicted = torch.max(output.data, 1)\n","            test_total += labels.size(0)\n","            test_correct += (predicted == labels).sum().item()\n","            # find loss\n","            loss = criterion(output, labels)\n","            validation_loss_history[epoch] += loss.item()\n","        validation_loss_history[epoch] /= len(test_data_loader)\n","        validation_accuracy_history[epoch] = test_correct / test_total\n","    print(f', val loss: {validation_loss_history[epoch,0]:0.4f}, val acc: {validation_accuracy_history[epoch,0]:0.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ImAgTZQrLsbx","executionInfo":{"status":"ok","timestamp":1706944989385,"user_tz":480,"elapsed":484961,"user":{"displayName":"Sujit Iyer","userId":"05651463945393873601"}},"outputId":"ec67b46b-723e-44f9-a9f9-b930fe6a837a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10:...........\n","\tloss: 0.0840, acc: 0.9734, val loss: 0.0431, val acc: 0.9857\n","Epoch 2/10:...........\n","\tloss: 0.0728, acc: 0.9777, val loss: 0.0347, val acc: 0.9886\n","Epoch 3/10:...........\n","\tloss: 0.0636, acc: 0.9806, val loss: 0.0279, val acc: 0.9897\n","Epoch 4/10:...........\n","\tloss: 0.0579, acc: 0.9815, val loss: 0.0386, val acc: 0.9882\n","Epoch 5/10:...........\n","\tloss: 0.0566, acc: 0.9820, val loss: 0.0368, val acc: 0.9871\n","Epoch 6/10:...........\n","\tloss: 0.0543, acc: 0.9833, val loss: 0.0260, val acc: 0.9917\n","Epoch 7/10:...........\n","\tloss: 0.0505, acc: 0.9848, val loss: 0.0342, val acc: 0.9898\n","Epoch 8/10:...........\n","\tloss: 0.0517, acc: 0.9843, val loss: 0.0310, val acc: 0.9915\n","Epoch 9/10:...........\n","\tloss: 0.0511, acc: 0.9838, val loss: 0.0253, val acc: 0.9918\n","Epoch 10/10:...........\n","\tloss: 0.0471, acc: 0.9859, val loss: 0.0252, val acc: 0.9916\n"]}]},{"cell_type":"markdown","source":["## Now that we have a working model, we will test out different Dropout parameters from 0.1 to 1"],"metadata":{"id":"JCHRTdf-12g1"}},{"cell_type":"code","source":["dropout_ps = np.linspace(0.1, 1, 10)\n","\n","for p in dropout_ps:\n","  model = nn.Sequential(\n","    nn.Conv2d(1, 32, kernel_size=(3,3)),\n","    nn.BatchNorm2d(num_features=32),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=p),\n","\n","    nn.Conv2d(32, 16, kernel_size=(3,3)),\n","    nn.BatchNorm2d(num_features=16),\n","    nn.ReLU(),\n","    nn.Dropout(p=p),\n","\n","    nn.Conv2d(16, 8, kernel_size=(3,3)),\n","    nn.BatchNorm2d(num_features=8),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=p),\n","\n","    nn.Flatten(),\n","    nn.Linear(128, 64),\n","    nn.BatchNorm1d(num_features=64),\n","    nn.ReLU(),\n","    nn.Linear(64, 10)\n","    # PyTorch implementation of cross-entropy loss includes softmax layer\n","  )\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.RMSprop(model.parameters())\n","\n","  # Train the model for 1 epoch, iterating on the data in batches\n","  n_epochs = 1\n","\n","  # store metrics\n","  training_accuracy_history = np.zeros([n_epochs, 1])\n","  training_loss_history = np.zeros([n_epochs, 1])\n","  validation_accuracy_history = np.zeros([n_epochs, 1])\n","  validation_loss_history = np.zeros([n_epochs, 1])\n","\n","  print(f'p = {p:.1f}:')\n","  for epoch in range(n_epochs):\n","    print(f'Epoch {epoch+1}/1:', end='')\n","    train_total = 0\n","    train_correct = 0\n","    # train\n","    model.train()\n","    for i, data in enumerate(training_data_loader):\n","        images, labels = data\n","        optimizer.zero_grad()\n","        # forward pass\n","        output = model(images)\n","        # calculate categorical cross entropy loss\n","        loss = criterion(output, labels)\n","        # backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # track training accuracy\n","        _, predicted = torch.max(output.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","        # track training loss\n","        training_loss_history[epoch] += loss.item()\n","        # progress update after 180 batches (~1/10 epoch for batch size 32)\n","        if i % 180 == 0: print('.',end='')\n","    training_loss_history[epoch] /= len(training_data_loader)\n","    training_accuracy_history[epoch] = train_correct / train_total\n","    print(f'\\n\\tloss: {training_loss_history[epoch,0]:0.4f}, acc: {training_accuracy_history[epoch,0]:0.4f}',end='')\n","\n","    # validate\n","    test_total = 0\n","    test_correct = 0\n","    with torch.no_grad():\n","        model.eval()\n","        for i, data in enumerate(test_data_loader):\n","            images, labels = data\n","            # forward pass\n","            output = model(images)\n","            # find accuracy\n","            _, predicted = torch.max(output.data, 1)\n","            test_total += labels.size(0)\n","            test_correct += (predicted == labels).sum().item()\n","            # find loss\n","            loss = criterion(output, labels)\n","            validation_loss_history[epoch] += loss.item()\n","        validation_loss_history[epoch] /= len(test_data_loader)\n","        validation_accuracy_history[epoch] = test_correct / test_total\n","    print(f', val loss: {validation_loss_history[epoch,0]:0.4f}, val acc: {validation_accuracy_history[epoch,0]:0.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qOkZbdBF1_6U","executionInfo":{"status":"ok","timestamp":1706946373170,"user_tz":480,"elapsed":636549,"user":{"displayName":"Sujit Iyer","userId":"05651463945393873601"}},"outputId":"ae8b00b6-bd12-4500-dcf5-67bca30ba774"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["p = 0.1:\n","Epoch 1/1:...........\n","\tloss: 0.1630, acc: 0.9486, val loss: 0.0477, val acc: 0.9842\n","p = 0.2:\n","Epoch 1/1:...........\n","\tloss: 0.1925, acc: 0.9394, val loss: 0.0528, val acc: 0.9840\n","p = 0.30000000000000004:\n","Epoch 1/1:...........\n","\tloss: 0.2515, acc: 0.9193, val loss: 0.0610, val acc: 0.9810\n","p = 0.4:\n","Epoch 1/1:...........\n","\tloss: 0.3138, acc: 0.8993, val loss: 0.0779, val acc: 0.9762\n","p = 0.5:\n","Epoch 1/1:...........\n","\tloss: 0.4329, acc: 0.8589, val loss: 0.0858, val acc: 0.9728\n","p = 0.6:\n","Epoch 1/1:...........\n","\tloss: 0.5757, acc: 0.8116, val loss: 0.1459, val acc: 0.9569\n","p = 0.7000000000000001:\n","Epoch 1/1:...........\n","\tloss: 0.8541, acc: 0.7152, val loss: 0.2156, val acc: 0.9414\n","p = 0.8:\n","Epoch 1/1:...........\n","\tloss: 1.1120, acc: 0.6193, val loss: 0.5164, val acc: 0.8697\n","p = 0.9:\n","Epoch 1/1:...........\n","\tloss: 1.6029, acc: 0.4387, val loss: 0.9492, val acc: 0.7899\n","p = 1.0:\n","Epoch 1/1:...........\n","\tloss: 2.3027, acc: 0.1105, val loss: 6159115.8291, val acc: 0.0975\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}